#!/usr/bin/env python3
"""
é–¢æ±æ¸©æ³‰ã®å®Ÿåœ¨ç”»åƒã‚’æ¤œç´¢ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒ•ãƒªãƒ¼ç”»åƒã‚µã‚¤ãƒˆå¯¾å¿œç‰ˆï¼‰

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ä»¥ä¸‹ã®æ–¹æ³•ã§ç”»åƒã‚’æ¤œç´¢ã—ã¾ã™ï¼š
1. Wikimedia Commons APIï¼ˆè‡ªå‹•ï¼‰
2. Unsplash APIï¼ˆAPIã‚­ãƒ¼ãŒå¿…è¦ãªå ´åˆï¼‰
3. Pexels APIï¼ˆAPIã‚­ãƒ¼ãŒå¿…è¦ãªå ´åˆï¼‰
4. Pixabay APIï¼ˆAPIã‚­ãƒ¼ãŒå¿…è¦ãªå ´åˆï¼‰
5. æ‰‹å‹•æ¤œç´¢ç”¨URLã®ç”Ÿæˆ

ãƒ†ãƒ¼ãƒå¤‰æ›´ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€content.jsonã‹ã‚‰æ¸©æ³‰æƒ…å ±ã‚’å‹•çš„ã«å–å¾—ã—ã¾ã™ã€‚
"""

import json
import os
import sys
import requests
from typing import List, Dict, Optional
from urllib.parse import quote

# æ—¢å­˜ç”»åƒã®URLãƒªã‚¹ãƒˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
EXISTING_IMAGE_URLS = set()

def load_existing_images():
    """æ—¢å­˜ã®ç”»åƒURLã‚’èª­ã¿è¾¼ã‚€"""
    global EXISTING_IMAGE_URLS
    
    # data/wikimedia-images.jsonã‹ã‚‰æ—¢å­˜URLã‚’èª­ã¿è¾¼ã¿
    try:
        with open('data/wikimedia-images.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            for key, value in data.items():
                if isinstance(value, dict) and 'url' in value:
                    EXISTING_IMAGE_URLS.add(value['url'])
    except FileNotFoundError:
        pass
    
    # app/lib/images.tsã‹ã‚‰æ—¢å­˜URLã‚’èª­ã¿è¾¼ã¿ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    try:
        with open('app/lib/images.ts', 'r', encoding='utf-8') as f:
            content = f.read()
            # URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
            import re
            urls = re.findall(r'https://[^\s"\'<>]+', content)
            EXISTING_IMAGE_URLS.update(urls)
    except FileNotFoundError:
        pass

def load_onsen_list_from_content():
    """content.jsonã‹ã‚‰æ¸©æ³‰ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open('themes/onsen-kanto/content.json', 'r', encoding='utf-8') as f:
            content = json.load(f)
        
        onsen_list = []
        
        # docsé…åˆ—ã‹ã‚‰æ¸©æ³‰æƒ…å ±ã‚’æŠ½å‡º
        if 'pages' in content and 'docs' in content['pages']:
            for doc in content['pages']['docs']:
                # onsenãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚‹å ´åˆ
                if 'onsen' in doc and isinstance(doc['onsen'], dict):
                    onsen_info = doc['onsen']
                    name = onsen_info.get('name', doc.get('title', ''))
                    name_en = onsen_info.get('nameEn', '')
                    prefecture = onsen_info.get('region', {}).get('prefecture', '')
                    
                    if name:
                        onsen_list.append({
                            'name': name,
                            'name_en': name_en,
                            'name_kana': onsen_info.get('nameKana', ''),
                            'prefecture': prefecture,
                            'slug': doc.get('slug', ''),
                            'keywords': [
                                name_en.lower() if name_en else '',
                                'onsen',
                                'rotenburo',
                                'outdoor bath',
                                'hot spring',
                                'japan'
                            ]
                        })
                # onsenãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãªã„å ´åˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¨æ¸¬ï¼‰
                elif 'title' in doc:
                    title = doc.get('title', '')
                    subtitle = doc.get('subtitle', '')
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¸©æ³‰åã‚’æŠ½å‡ºï¼ˆä¾‹: "ç®±æ ¹æ¸©æ³‰éƒ·å®Œå…¨ã‚¬ã‚¤ãƒ‰" -> "ç®±æ ¹æ¸©æ³‰éƒ·"ï¼‰
                    if 'æ¸©æ³‰' in title:
                        # "å®Œå…¨ã‚¬ã‚¤ãƒ‰"ãªã©ã‚’é™¤å»
                        name = title.replace('å®Œå…¨ã‚¬ã‚¤ãƒ‰', '').replace('ã‚¬ã‚¤ãƒ‰', '').strip()
                        # subtitleã‹ã‚‰éƒ½é“åºœçœŒã‚’æŠ½å‡º
                        prefecture = ''
                        if 'çœŒ' in subtitle or 'éƒ½' in subtitle or 'åºœ' in subtitle:
                            parts = subtitle.split(' - ')
                            if len(parts) > 0:
                                prefecture = parts[0].strip()
                        
                        if name:
                            onsen_list.append({
                                'name': name,
                                'name_en': '',
                                'name_kana': '',
                                'prefecture': prefecture,
                                'slug': doc.get('slug', ''),
                                'keywords': [
                                    'onsen',
                                    'rotenburo',
                                    'outdoor bath',
                                    'hot spring',
                                    'japan'
                                ]
                            })
        
        return onsen_list
    except Exception as e:
        print(f"Error loading content.json: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return []

def search_wikimedia_commons(query: str, limit: int = 10) -> List[Dict]:
    """Wikimedia Commons APIã§ç”»åƒã‚’æ¤œç´¢"""
    results = []
    
    try:
        # Wikimedia Commons API
        api_url = "https://commons.wikimedia.org/w/api.php"
        headers = {
            'User-Agent': 'OnsenImageSearchBot/1.0 (https://github.com/kazu-4728/website_v2)'
        }
        params = {
            'action': 'query',
            'format': 'json',
            'list': 'search',
            'srsearch': query,
            'srnamespace': 6,  # File namespace
            'srlimit': limit,
            'srprop': 'size|wordcount|timestamp',
        }
        
        response = requests.get(api_url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if 'query' in data and 'search' in data['query']:
            for item in data['query']['search']:
                title = item.get('title', '')
                if title.startswith('File:'):
                    filename = title[5:]  # Remove "File:" prefix
                    
                    # PDFã‚„ãã®ä»–ã®éç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if filename.lower().endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx')):
                        continue
                    
                    # ç”»åƒURLã‚’æ§‹ç¯‰ï¼ˆWikimedia Commonsã®URLæ§‹é€ ã«å¾“ã†ï¼‰
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®æœ€åˆã®1æ–‡å­—ã¨æœ€åˆã®2æ–‡å­—ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œã‚‹
                    safe_filename = filename.replace(' ', '_')
                    if len(safe_filename) >= 2:
                        dir1 = safe_filename[0]
                        dir2 = safe_filename[0:2] if len(safe_filename) >= 2 else safe_filename[0]
                        encoded_filename = quote(safe_filename)
                        image_url = f"https://upload.wikimedia.org/wikipedia/commons/{dir1}/{dir2}/{encoded_filename}"
                    else:
                        encoded_filename = quote(safe_filename)
                        image_url = f"https://upload.wikimedia.org/wikipedia/commons/{encoded_filename}"
                    
                    # æ—¢å­˜ç”»åƒã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                    if image_url not in EXISTING_IMAGE_URLS:
                        results.append({
                            'url': image_url,
                            'title': filename,
                            'source': 'wikimedia',
                            'source_url': f"https://commons.wikimedia.org/wiki/{title}",
                            'license': 'CC BY-SA / CC BY (è¦ç¢ºèª)',
                            'photographer': 'è¦ç¢ºèª',
                            'description': f"Wikimedia Commons: {filename}"
                        })
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œï¼ˆæ‰‹å‹•æ¤œç´¢URLã¯ç”Ÿæˆã•ã‚Œã‚‹ï¼‰
        pass
    
    return results

def generate_search_urls(onsen_name_jp: str, onsen_name_en: str, prefecture: str) -> Dict[str, str]:
    """å„ãƒ•ãƒªãƒ¼ç”»åƒã‚µã‚¤ãƒˆã®æ¤œç´¢URLã‚’ç”Ÿæˆ"""
    urls = {}
    
    # æ—¥æœ¬èªã‚¯ã‚¨ãƒª
    queries_jp = [
        f"{onsen_name_jp} æ¸©æ³‰ éœ²å¤©é¢¨å‘‚",
        f"{onsen_name_jp} æ¸©æ³‰ ç„¡æ–™ç”»åƒ",
        f"{prefecture} {onsen_name_jp} æ¸©æ³‰",
    ]
    
    # è‹±èªã‚¯ã‚¨ãƒªï¼ˆname_enãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    queries_en = []
    if onsen_name_en and onsen_name_en.strip():
        queries_en = [
            f"{onsen_name_en} onsen rotenburo",
            f"{onsen_name_en} onsen free image",
        ]
    
    # Googleæ¤œç´¢URLï¼ˆæ—¥æœ¬èªï¼‰
    for query in queries_jp:
        if query.strip():
            encoded_query = quote(query)
            key = f"google_{query[:20].replace(' ', '_')}"
            urls[key] = f"https://www.google.com/search?q={encoded_query}&tbm=isch&tbs=sur:fc"
    
    # Googleæ¤œç´¢URLï¼ˆè‹±èªï¼‰
    for query in queries_en:
        if query.strip():
            encoded_query = quote(query)
            key = f"google_{query[:20].replace(' ', '_')}"
            urls[key] = f"https://www.google.com/search?q={encoded_query}&tbm=isch&tbs=sur:fc"
    
    # å„ãƒ•ãƒªãƒ¼ç”»åƒã‚µã‚¤ãƒˆã®æ¤œç´¢URLï¼ˆæ—¥æœ¬èªï¼‰
    base_query_jp = f"{onsen_name_jp} æ¸©æ³‰"
    if base_query_jp.strip():
        encoded_query = quote(base_query_jp)
        urls[f"unsplash_jp"] = f"https://unsplash.com/s/photos/{encoded_query}"
        urls[f"pexels_jp"] = f"https://www.pexels.com/search/{encoded_query}/"
        urls[f"pixabay_jp"] = f"https://pixabay.com/images/search/{encoded_query}/"
        urls[f"wikimedia_jp"] = f"https://commons.wikimedia.org/wiki/Category:{encoded_query}"
    
    # å„ãƒ•ãƒªãƒ¼ç”»åƒã‚µã‚¤ãƒˆã®æ¤œç´¢URLï¼ˆè‹±èªï¼‰
    if onsen_name_en and onsen_name_en.strip():
        base_query_en = f"{onsen_name_en} onsen"
        encoded_query = quote(base_query_en)
        urls[f"unsplash_en"] = f"https://unsplash.com/s/photos/{encoded_query}"
        urls[f"pexels_en"] = f"https://www.pexels.com/search/{encoded_query}/"
        urls[f"pixabay_en"] = f"https://pixabay.com/images/search/{encoded_query}/"
        urls[f"wikimedia_en"] = f"https://commons.wikimedia.org/wiki/Category:{encoded_query}"
    
    return urls

def format_image_result(image: Dict, onsen_name: str) -> str:
    """ç”»åƒçµæœã‚’Markdownå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    lines = [
        f"### {onsen_name} - {image.get('title', 'Unknown')}",
        f"",
        f"- **URL**: `{image['url']}`",
        f"- **ã‚½ãƒ¼ã‚¹**: {image.get('source', 'Unknown')}",
        f"- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: {image.get('license', 'è¦ç¢ºèª')}",
        f"- **æ’®å½±è€…**: {image.get('photographer', 'è¦ç¢ºèª')}",
        f"- **èª¬æ˜**: {image.get('description', '')}",
        f"- **ã‚½ãƒ¼ã‚¹URL**: {image.get('source_url', '')}",
        f"",
        f"---",
        f"",
    ]
    return "\n".join(lines)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("# é–¢æ±æ¸©æ³‰ã®å®Ÿåœ¨ç”»åƒæ¤œç´¢çµæœï¼ˆè‡ªå‹•æ¤œç´¢ + æ‰‹å‹•æ¤œç´¢URLï¼‰\n")
    print("æœ€çµ‚æ›´æ–°: 2025-01-XX\n")
    print("## ğŸ“‹ æ¤œç´¢æ–¹é‡\n")
    print("ä»¥ä¸‹ã®æ¡ä»¶ã§å„æ¸©æ³‰ã®ç”»åƒã‚’æ¤œç´¢ã—ã¾ã™ï¼š")
    print("1. **å®Ÿéš›ã®æ¸©æ³‰ã€ãŠæ¹¯ãŒæ˜ ã£ã¦ã„ã‚‹**ï¼ˆæ¹¯èˆ¹ãƒ»éœ²å¤©é¢¨å‘‚ãŒæ˜ç¢ºã«å†™ã£ã¦ã„ã‚‹ï¼‰")
    print("2. **é–¢æ±ã®æ¸©æ³‰**")
    print("3. **æ—¢å­˜ç”»åƒã¯ä½¿ç”¨ã—ãªã„**")
    print("4. **ã©ã“ã®æ¸©æ³‰ã‹åˆ†ã‹ã‚‹ã‚ˆã†ã«ã™ã‚‹**ï¼ˆæ¸©æ³‰åãƒ»å ´æ‰€ãŒç‰¹å®šã§ãã‚‹ï¼‰\n")
    print("---\n")
    
    # æ—¢å­˜ç”»åƒã‚’èª­ã¿è¾¼ã‚€
    load_existing_images()
    print(f"æ—¢å­˜ç”»åƒæ•°: {len(EXISTING_IMAGE_URLS)}\n")
    
    # æ¸©æ³‰ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
    onsen_list = load_onsen_list_from_content()
    
    if not onsen_list:
        print("âš ï¸ æ¸©æ³‰ãƒªã‚¹ãƒˆãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚", file=sys.stderr)
        return
    
    print(f"æ¤œç´¢å¯¾è±¡æ¸©æ³‰æ•°: {len(onsen_list)}\n")
    print("---\n")
    
    # å„æ¸©æ³‰ã«ã¤ã„ã¦æ¤œç´¢
    all_results = []
    
    for onsen in onsen_list:
        name_jp = onsen['name']
        name_en = onsen.get('name_en', '')
        prefecture = onsen.get('prefecture', '')
        
        print(f"## ğŸ” {name_jp} ({prefecture})\n")
        
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        search_query = f"{name_jp} æ¸©æ³‰ éœ²å¤©é¢¨å‘‚" if name_jp else f"{name_en} onsen rotenburo"
        
        # Wikimedia Commonsã§è‡ªå‹•æ¤œç´¢
        print(f"### è‡ªå‹•æ¤œç´¢çµæœï¼ˆWikimedia Commonsï¼‰\n")
        wikimedia_results = search_wikimedia_commons(search_query, limit=5)
        
        if wikimedia_results:
            for img in wikimedia_results:
                print(format_image_result(img, name_jp))
                all_results.append({
                    'onsen': name_jp,
                    'image': img
                })
        else:
            print("è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n")
        
        # æ‰‹å‹•æ¤œç´¢ç”¨URLã‚’ç”Ÿæˆ
        print(f"### æ‰‹å‹•æ¤œç´¢ç”¨URL\n")
        search_urls = generate_search_urls(name_jp, name_en, prefecture)
        
        for key, url in search_urls.items():
            print(f"- **{key}**: {url}")
        
        print("\n---\n")
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = 'docs/reports/ONSEN_IMAGE_SEARCH_RESULTS_AUTO.json'
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'total_onsen': len(onsen_list),
                'total_images': len(all_results),
                'results': all_results
            }, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\n")
    except Exception as e:
        print(f"âš ï¸ çµæœã®ä¿å­˜ã«å¤±æ•—: {e}", file=sys.stderr)

if __name__ == '__main__':
    main()
